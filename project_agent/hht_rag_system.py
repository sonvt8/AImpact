import os
import sys
import streamlit as st
import chromadb
from chromadb.config import Settings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from typing import List
import pdfplumber
import shutil
from datetime import datetime
from functools import lru_cache
import hashlib
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_community.chat_message_histories import SQLChatMessageHistory
from cryptography.fernet import Fernet
import requests
import warnings
import torch
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from reportlab.lib.units import inch
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.pdfbase import pdfmetrics
import io

warnings.filterwarnings("ignore", category=UserWarning, module="torch")

if sys.version_info >= (3, 13):
    st.error("Python 3.13 kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. Vui l√≤ng s·ª≠ d·ª•ng Python 3.11 ho·∫∑c 3.12.")
    st.stop()

if torch.__version__ < "2.3.1":
    st.error(f"Phi√™n b·∫£n torch ({torch.__version__}) kh√¥ng ƒë·ªß m·ªõi. Vui l√≤ng c√†i torch>=2.3.1.")
    st.stop()

def check_ollama():
    try:
        response = requests.get("http://localhost:11434", timeout=5)
        return response.status_code == 200
    except:
        return False

DATA_DIR = "data"
HISTORY_DIR = "history"
DOCUMENTS_DIR = "Documents"
CHROMA_DB_PATH = os.path.join(DATA_DIR, "chroma_db")
ENCRYPTION_KEY_PATH = os.path.join(DATA_DIR, "encryption_key.key")
HISTORY_DB_PATH = os.path.join(HISTORY_DIR, "query_history.db")
MODEL_PATH = "./models/all-MiniLM-L6-v2"

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(HISTORY_DIR, exist_ok=True)
os.makedirs(DOCUMENTS_DIR, exist_ok=True)

def display_message(message, message_type="error"):
    if "message_placeholder" not in st.session_state:
        st.session_state.message_placeholder = st.empty()

    if "last_message" not in st.session_state or st.session_state.last_message != message:
        st.session_state.last_message = message
        message_id = f"message_{hashlib.md5(message.encode()).hexdigest()}"

        color = {
            "error": "#ff4b4b",
            "warning": "#ffcc00",
            "info": "#28a745"
        }.get(message_type, "#ff4b4b")

        html_message = f"""
        <div id="{message_id}" style="padding: 10px; margin-bottom: 10px; border-radius: 5px; color: white; 
        background-color: {color};">
            {message}
        </div>
        <script>
            setTimeout(function() {{
                var elem = document.getElementById("{message_id}");
                if (elem) {{
                    elem.parentNode.removeChild(elem);
                }}
            }}, 5000);
        </script>
        """

        st.session_state.message_placeholder.markdown(html_message, unsafe_allow_html=True)

def check_existing_data(persist_directory: str = CHROMA_DB_PATH):
    return os.path.exists(persist_directory) and len(os.listdir(persist_directory)) > 0

class DocumentProcessor:
    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 100):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
        )

    def load_and_split(self, pdf_file) -> List[str]:
        saved_filename = pdf_file.name
        saved_filepath = os.path.join(DOCUMENTS_DIR, saved_filename)

        if not os.path.exists(saved_filepath):
            with open(saved_filepath, "wb") as f:
                f.write(pdf_file.read())

        chunks = []
        try:
            with pdfplumber.open(saved_filepath) as pdf:
                for page in pdf.pages:
                    text = page.extract_text()
                    if text:
                        chunks.append(text.strip())
                    tables = page.extract_tables()
                    for table in tables:
                        table_text = "\n".join([",".join(str(cell) if cell is not None else "" for cell in row) for row in table])
                        if table_text.strip():
                            chunks.append(table_text.strip())
        except Exception as e:
            display_message(f"L·ªói x·ª≠ l√Ω PDF: {str(e)}", "error")
            return []

        splits = self.text_splitter.create_documents(chunks)
        return [chunk.page_content.strip() for chunk in splits if chunk.page_content.strip()]

class SentenceEmbedding:
    def __init__(self, model_path: str = MODEL_PATH):
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        if not os.path.exists(model_path):
            display_message(f"M√¥ h√¨nh 'all-MiniLM-L6-v2' kh√¥ng t√¨m th·∫•y t·∫°i {model_path}.", "error")
            display_message("Vui l√≤ng ch·∫°y: from sentence_transformers import SentenceTransformer; model = SentenceTransformer('all-MiniLM-L6-v2'); model.save('./models/all-MiniLM-L6-v2')", "info")
            st.stop()
        try:
            self.model = SentenceTransformer(model_path)
        except Exception as e:
            display_message(f"L·ªói t·∫£i m√¥ h√¨nh sentence-transformers: {str(e)}", "error")
            st.stop()

    def __call__(self, texts: List[str]) -> List[List[float]]:
        try:
            return self.model.encode(texts, show_progress_bar=False).tolist()
        except Exception as e:
            display_message(f"L·ªói t·∫°o embedding: {str(e)}", "error")
            return []

class ChromaDBManager:
    def __init__(self, persist_directory: str = CHROMA_DB_PATH):
        self.key_path = ENCRYPTION_KEY_PATH
        os.makedirs(os.path.dirname(self.key_path), exist_ok=True)
        if os.path.exists(self.key_path):
            with open(self.key_path, "rb") as f:
                self.key = f.read()
        else:
            self.key = Fernet.generate_key()
            with open(self.key_path, "wb") as f:
                f.write(self.key)
        self.cipher = Fernet(self.key)
        os.makedirs(persist_directory, exist_ok=True)
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.collection = self.client.get_or_create_collection(name="viettel_docs")

    def add(self, texts: List[str], embeddings: List[List[float]], filename: str):
        encrypted_texts = [self.cipher.encrypt(text.encode()).decode() for text in texts]
        ids = [f"doc_{hashlib.md5((filename + str(i)).encode()).hexdigest()}" for i in range(len(texts))]
        metas = [{"source": f"ƒêo·∫°n {i+1}", "filename": filename, "upload_date": datetime.now().isoformat()} for i in range(len(texts))]
        self.collection.add(
            ids=ids,
            documents=encrypted_texts,
            metadatas=metas,
            embeddings=embeddings,
        )

    def query(self, query_embedding: List[float], top_k: int = 3):
        res = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            include=["documents", "metadatas", "distances"]
        )
        res["documents"] = [[self.cipher.decrypt(doc.encode()).decode() for doc in docs] for docs in res["documents"]]
        return res

class AnswerGenerator:
    def __init__(self, model_type: str, role: str = "Expert"):
        self.model_type = model_type
        self.role = role
        if model_type == "openai":
            try:
                self.model = ChatOpenAI(model="gpt-4o-mini")
            except Exception as e:
                display_message(f"L·ªói k·∫øt n·ªëi OpenAI: {str(e)}. Vui l√≤ng ki·ªÉm tra API key ho·∫∑c k·∫øt n·ªëi internet.", "error")
                st.stop()
        elif model_type == "ollama":
            if not check_ollama():
                display_message("Ollama server kh√¥ng ho·∫°t ƒë·ªông. Vui l√≤ng ch·∫°y 'ollama run llama3.2'.", "error")
                st.stop()
            self.model = ChatOllama(
                base_url="http://localhost:11434",
                model="llama3.2",
                temperature=0.3,
                max_tokens=2000
            )
        else:
            raise ValueError(f"Unsupported model type: {model_type}")

    def generate_answer(self, question: str, context: str, citations: str) -> str:
        prompt = PromptTemplate(
            input_variables=["question", "context", "citations", "role"],
            template="""You are an expert in telecommunications and Data Center operations, answering at a {role} level. Given the following question, context, and citations, provide a concise, accurate, and technical answer using domain-specific terminology:

            Question: {question}

            Context: {context}

            Citations: {citations}

            Answer:"""
        )
        try:
            formatted_prompt = prompt.format(question=question, context=context, citations=citations, role=self.role)
            response = self.model.invoke(formatted_prompt)
            return response.content.strip()
        except Exception as e:
            display_message(f"Error generating answer: {str(e)}", "error")
            return ""

def get_session_history(session_id: str):
    return SQLChatMessageHistory(session_id=session_id, connection=f"sqlite:///{HISTORY_DB_PATH}")

def wrap_text(text, width, canvas_obj, font_name="Helvetica", font_size=12):
    """H√†m ƒë·ªÉ b·ªçc vƒÉn b·∫£n trong PDF n·∫øu v∆∞·ª£t qu√° chi·ªÅu r·ªông."""
    lines = []
    current_line = ""
    canvas_obj.setFont(font_name, font_size)
    for word in text.split():
        test_line = current_line + word + " "
        if canvas_obj.stringWidth(test_line, font_name, font_size) <= width:
            current_line = test_line
        else:
            lines.append(current_line.strip())
            current_line = word + " "
    if current_line:
        lines.append(current_line.strip())
    return lines

def main():
    st.set_page_config(page_title="RAG ƒêi·ªán Vi·ªÖn Th√¥ng (N·ªôi b·ªô)", layout="wide")

    if "message_placeholder" not in st.session_state:
        st.session_state.message_placeholder = st.empty()

    st.title("üìÑ H·ªá th·ªëng truy v·∫•n t√†i li·ªáu th√¥ng minh")

    if "authenticated" not in st.session_state:
        st.session_state.authenticated = False
    if not st.session_state.authenticated:
        password = st.text_input("Nh·∫≠p m·∫≠t kh·∫©u:", type="password", value="")
        if st.button("X√°c th·ª±c"):
            if password == "T0mmy":
                st.session_state.authenticated = True
                st.rerun()
            else:
                display_message("M·∫≠t kh·∫©u kh√¥ng ƒë√∫ng. Vui l√≤ng th·ª≠ l·∫°i.", "error")
        return

    if "processor" not in st.session_state:
        st.session_state.processor = None
    if "embedder" not in st.session_state:
        st.session_state.embedder = None
    if "chroma" not in st.session_state:
        st.session_state.chroma = None
    if "query_history" not in st.session_state:
        st.session_state.query_history = get_session_history("user_default")
    if "has_db_notified" not in st.session_state:
        st.session_state.has_db_notified = False

    st.sidebar.header("C·∫•u h√¨nh")
    llm_type = st.sidebar.radio(
        "Ch·ªçn m√¥ h√¨nh LLM:",
        ["ollama", "openai"],
        format_func=lambda x: "Ollama Llama3.2 (Offline)" if x == "ollama" else "OpenAI GPT-4o-mini (Online)",
    )
    role = st.sidebar.radio("M·ª©c ƒë·ªô chi ti·∫øt:", ["Beginner", "Expert", "PhD"])

    has_db = check_existing_data()
    if has_db:
        display_message("CSDL ƒë√£ c√≥ d·ªØ li·ªáu. B·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu truy v·∫•n.", "info")
        # st.sidebar.info("CSDL ƒë√£ c√≥ d·ªØ li·ªáu. B·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu truy v·∫•n.")
        if st.session_state.chroma is None:
            st.session_state.chroma = ChromaDBManager(persist_directory=CHROMA_DB_PATH)
        if st.session_state.embedder is None:
            st.session_state.embedder = SentenceEmbedding(model_path=MODEL_PATH)
        st.session_state.has_db_notified = False
    else:
        if not st.session_state.has_db_notified:
            display_message("Ch∆∞a c√≥ d·ªØ li·ªáu. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu PDF.", "warning")
            st.session_state.has_db_notified = True

    st.sidebar.subheader("Qu·∫£n l√Ω t√†i li·ªáu")
    pdf_file = st.sidebar.file_uploader("T·∫£i l√™n file PDF", type="pdf")
    if pdf_file:
        with st.spinner("ƒêang x·ª≠ l√Ω t√†i li·ªáu‚Ä¶"):
            try:
                proc = DocumentProcessor()
                chunks = proc.load_and_split(pdf_file)
                if not chunks:
                    display_message("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ PDF.", "error")
                    return
                embedder = SentenceEmbedding(model_path=MODEL_PATH)
                embeddings = embedder(chunks)
                chroma = ChromaDBManager(persist_directory=CHROMA_DB_PATH)
                chroma.add(chunks, embeddings, filename=pdf_file.name)
                st.session_state.processor = proc
                st.session_state.embedder = embedder
                st.session_state.chroma = chroma
                display_message("ƒê√£ x·ª≠ l√Ω v√† l∆∞u t√†i li·ªáu th√†nh c√¥ng!", "info")
                st.session_state.has_db_notified = False
            except Exception as e:
                display_message(f"L·ªói x·ª≠ l√Ω t√†i li·ªáu: {str(e)}", "error")

    if has_db and st.sidebar.button("X√≥a t·∫•t c·∫£ t√†i li·ªáu"):
        try:
            st.session_state.chroma.client.delete_collection("viettel_docs")
            st.session_state.chroma = None
            st.session_state.embedder = None
            st.session_state.processor = None
            for file in os.listdir(DOCUMENTS_DIR):
                file_path = os.path.join(DOCUMENTS_DIR, file)
                if os.path.isfile(file_path):
                    os.remove(file_path)
            display_message("ƒê√£ x√≥a t·∫•t c·∫£ t√†i li·ªáu!", "info")
            st.session_state.has_db_notified = False
        except Exception as e:
            display_message(f"L·ªói x√≥a t√†i li·ªáu: {str(e)}", "error")

    query = st.text_input("Nh·∫≠p c√¢u h·ªèi (ti·∫øng Vi·ªát):")
    if query:
        if st.session_state.chroma is None or st.session_state.embedder is None:
            display_message("Ch∆∞a c√≥ d·ªØ li·ªáu. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu PDF.", "warning")
            return
        with st.spinner("ƒêang t·∫°o c√¢u tr·∫£ l·ªùi‚Ä¶"):
            try:
                @lru_cache(maxsize=100)
                def cached_query(query: str, top_k: int) -> tuple:
                    emb = st.session_state.embedder([query])[0]
                    res = st.session_state.chroma.query(emb, top_k)
                    return res["documents"][0], res["metadatas"][0], res["distances"][0]

                docs, metas, dists = cached_query(query, top_k=3)
                context = " ".join(docs)
                citations = "\n".join([f"**{meta['source']}** ({meta['filename']}, score={dist:.4f}):\n> {doc}" for meta, doc, dist in zip(metas, docs, dists)])

                answer_generator = AnswerGenerator(model_type=llm_type, role=role)
                final_answer = answer_generator.generate_answer(query, context, citations)

                if final_answer.startswith("Error generating answer"):
                    display_message(final_answer, "error")
                    return

                st.session_state.query_history.add_user_message(query)
                st.session_state.query_history.add_ai_message(final_answer)

                st.markdown("### C√¢u tr·∫£ l·ªùi cu·ªëi c√πng:")
                st.write(final_answer)

                with st.expander("üìö Xem tr√≠ch d·∫´n ngu·ªìn"):
                    st.markdown(citations, unsafe_allow_html=True)

                if st.button("Xu·∫•t c√¢u tr·∫£ l·ªùi"):
                    buffer = io.BytesIO()
                    c = canvas.Canvas(buffer, pagesize=letter)
                    width, height = letter

                    # Ki·ªÉm tra font DejaVuSans
                    font_name = "Helvetica"  # Font m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng t√¨m th·∫•y DejaVuSans
                    font_available = os.path.exists("DejaVuSans.ttf")
                    if font_available:
                        try:
                            pdfmetrics.registerFont(TTFont("DejaVuSans", "DejaVuSans.ttf"))
                            font_name = "DejaVuSans"
                        except Exception as e:
                            display_message(f"L·ªói t·∫£i font DejaVuSans: {str(e)}. S·∫Ω s·ª≠ d·ª•ng font m·∫∑c ƒë·ªãnh (Helvetica). Ti·∫øng Vi·ªát c√≥ th·ªÉ kh√¥ng hi·ªÉn th·ªã ƒë√∫ng.", "warning")
                    else:
                        display_message(
                            "Kh√¥ng t√¨m th·∫•y file font DejaVuSans.ttf. S·∫Ω s·ª≠ d·ª•ng font m·∫∑c ƒë·ªãnh (Helvetica). "
                            "Ti·∫øng Vi·ªát c√≥ th·ªÉ kh√¥ng hi·ªÉn th·ªã ƒë√∫ng. Vui l√≤ng t·∫£i font DejaVuSans.ttf v√† ƒë·∫∑t v√†o th∆∞ m·ª•c d·ª± √°n.",
                            "warning"
                        )

                    y_position = height - inch  # V·ªã tr√≠ b·∫Øt ƒë·∫ßu t·ª´ tr√™n xu·ªëng

                    # Ti√™u ƒë·ªÅ: C√¢u h·ªèi
                    c.setFont(font_name, 16)
                    c.drawString(inch, y_position, "C√¢u h·ªèi:")
                    y_position -= 0.5 * inch

                    # N·ªôi dung c√¢u h·ªèi
                    c.setFont(font_name, 12)
                    query_lines = wrap_text(query, width - 2 * inch, c, font_name, 12)
                    for line in query_lines:
                        if y_position < inch:
                            c.showPage()
                            c.setFont(font_name, 12)
                            y_position = height - inch
                        c.drawString(inch, y_position, line)
                        y_position -= 0.3 * inch

                    # Ti√™u ƒë·ªÅ: C√¢u tr·∫£ l·ªùi
                    y_position -= 0.5 * inch
                    if y_position < inch:
                        c.showPage()
                        c.setFont(font_name, 16)
                        y_position = height - inch
                    c.setFont(font_name, 16)
                    c.drawString(inch, y_position, "C√¢u tr·∫£ l·ªùi:")
                    y_position -= 0.5 * inch

                    # N·ªôi dung c√¢u tr·∫£ l·ªùi
                    c.setFont(font_name, 12)
                    answer_lines = wrap_text(final_answer, width - 2 * inch, c, font_name, 12)
                    for line in answer_lines:
                        if y_position < inch:
                            c.showPage()
                            c.setFont(font_name, 12)
                            y_position = height - inch
                        c.drawString(inch, y_position, line)
                        y_position -= 0.3 * inch

                    # Ti√™u ƒë·ªÅ: Tr√≠ch d·∫´n
                    y_position -= 0.5 * inch
                    if y_position < inch:
                        c.showPage()
                        c.setFont(font_name, 16)
                        y_position = height - inch
                    c.setFont(font_name, 16)
                    c.drawString(inch, y_position, "Tr√≠ch d·∫´n:")
                    y_position -= 0.5 * inch

                    # N·ªôi dung tr√≠ch d·∫´n
                    c.setFont(font_name, 12)
                    citations_lines = wrap_text(citations, width - 2 * inch, c, font_name, 12)
                    for line in citations_lines:
                        if y_position < inch:
                            c.showPage()
                            c.setFont(font_name, 12)
                            y_position = height - inch
                        c.drawString(inch, y_position, line)
                        y_position -= 0.3 * inch

                    c.save()
                    buffer.seek(0)
                    st.download_button("T·∫£i PDF", buffer, file_name="answer.pdf", mime="application/pdf")
            except Exception as e:
                display_message(f"L·ªói truy v·∫•n: {str(e)}", "error")

    with st.expander("üìú L·ªãch s·ª≠ truy v·∫•n"):
        messages = st.session_state.query_history.messages
        for i in range(0, len(messages), 2):
            if i + 1 < len(messages):
                user_msg = messages[i]
                ai_msg = messages[i + 1]
                user_content = user_msg.content.replace("\n", "<br>")
                ai_content = ai_msg.content.replace("\n", "<br>")
                st.markdown(
                    """
                    <div style="background-color: #ffffff; padding: 10px; border-radius: 5px; margin-bottom: 10px;">
                        üë§ <strong>Ng∆∞·ªùi d√πng:</strong> {user_content}
                    </div>
                    """.format(user_content=user_content),
                    unsafe_allow_html=True
                )
                st.markdown(
                    """
                    <div style="background-color: #d3d3d3; padding: 10px; border-radius: 5px; margin-bottom: 10px;">
                        ü§ñ <strong>Tr·ª£ l√Ω:</strong> {ai_content}
                    </div>
                    """.format(ai_content=ai_content),
                    unsafe_allow_html=True
                )
                if i + 2 < len(messages):
                    st.markdown("---")
            else:
                user_msg = messages[i]
                user_content = user_msg.content.replace("\n", "<br>")
                st.markdown(
                    """
                    <div style="background-color: #ffffff; padding: 10px; border-radius: 5px; margin-bottom: 10px;">
                        üë§ <strong>Ng∆∞·ªùi d√πng:</strong> {user_content}
                    </div>
                    """.format(user_content=user_content),
                    unsafe_allow_html=True
                )

if __name__ == "__main__":
    main()