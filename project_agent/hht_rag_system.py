import os
import sys
import streamlit as st
import chromadb
from chromadb.config import Settings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from typing import List
import pdfplumber
import shutil
from datetime import datetime
from functools import lru_cache
import hashlib
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_community.chat_message_histories import SQLChatMessageHistory
from cryptography.fernet import Fernet
import requests
import warnings
import torch
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from reportlab.lib.units import inch
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.pdfbase import pdfmetrics
import io
import re
import numpy as np
import logging
import unicodedata
import pytesseract
from PIL import Image
import io

logging.basicConfig(level=logging.INFO, filename='rag_system.log', filemode='a',
                    format='%(asctime)s - %(levelname)s - %(message)s')

warnings.filterwarnings("ignore", category=UserWarning, module="torch")

if sys.version_info >= (3, 13):
    st.error("Python 3.13 kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. Vui l√≤ng s·ª≠ d·ª•ng Python 3.11 ho·∫∑c 3.12.")
    st.stop()

if torch.__version__ < "2.3.1":
    st.error(f"Phi√™n b·∫£n torch ({torch.__version__}) kh√¥ng ƒë·ªß m·ªõi. Vui l√≤ng c√†i torch>=2.3.1.")
    st.stop()

def check_ollama():
    try:
        response = requests.get("http://localhost:11434", timeout=5)
        return response.status_code == 200
    except:
        return False

DATA_DIR = "data"
HISTORY_DIR = "history"
DOCUMENTS_DIR = "Documents"
CHROMA_DB_PATH = os.path.join(DATA_DIR, "chroma_db")
ENCRYPTION_KEY_PATH = os.path.join(DATA_DIR, "encryption_key.key")
HISTORY_DB_PATH = os.path.join(HISTORY_DIR, "query_history.db")
MODEL_PATH = "./models/multilingual-e5-large"

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(HISTORY_DIR, exist_ok=True)
os.makedirs(DOCUMENTS_DIR, exist_ok=True)

def display_message(message, message_type="error"):
    if "message_placeholder" not in st.session_state:
        st.session_state.message_placeholder = st.empty()

    if "last_message" not in st.session_state or st.session_state.last_message != message:
        st.session_state.last_message = message
        message_id = f"message_{hashlib.md5(message.encode()).hexdigest()}"

        color = {
            "error": "#ff4b4b",
            "warning": "#ffcc00",
            "info": "#28a745"
        }.get(message_type, "#ff4b4b")

        html_message = f"""
        <div id="{message_id}" style="padding: 10px; margin-bottom: 10px; border-radius: 5px; color: white; 
        background-color: {color};">
            {message}
        </div>
        <script>
            setTimeout(function() {{
                var elem = document.getElementById("{message_id}");
                if (elem) {{
                    elem.parentNode.removeChild(elem);
                }}
            }}, 5000);
        </script>
        """

        st.session_state.message_placeholder.markdown(html_message, unsafe_allow_html=True)

def check_existing_data(persist_directory: str = CHROMA_DB_PATH):
    return os.path.exists(persist_directory) and len(os.listdir(persist_directory)) > 0

# Danh s√°ch t·ª´ ng·ªØ chuy√™n ng√†nh vi·ªÖn th√¥ng (c√≥ th·ªÉ t√πy ch·ªânh)
TELECOM_KEYWORDS = [
    "bƒÉng th√¥ng", "h·∫° t·∫ßng m·∫°ng", "ƒë·ªãnh tuy·∫øn", "giao th·ª©c", "t·∫ßn s·ªë", "k√™nh truy·ªÅn",
    "ƒë·ªìng b·ªô", "ƒë·ªô tr·ªÖ", "t·ªëc ƒë·ªô truy·ªÅn", "m·∫°ng 5G", "IP", "MPLS", "VPN", "firewall",
    "ƒë√°m m√¢y", "trung t√¢m d·ªØ li·ªáu", "Data Center", "c√°p quang", "vi·ªÖn th√¥ng", "tr·∫°m BTS",
    "ACB", "t·ªß ƒëi·ªán", "m√°y ph√°t", "SYN", "ATS", "LVSB", "LVGB"
]

def preprocess_text(text: str) -> str:
    """Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: lo·∫°i b·ªè nhi·ªÖu v√† s·ª≠a l·ªói OCR ti·∫øng Vi·ªát."""
    original_text = text  # L∆∞u b·∫£n g·ªëc ƒë·ªÉ log

    # Lo·∫°i b·ªè nhi·ªÖu t·ª´ s∆° ƒë·ªì/b·∫£ng bi·ªÉu
    text = re.sub(r'\b[-=]{3,}\b', ' ', text)  # Lo·∫°i b·ªè c√°c chu·ªói nh∆∞ "----", "===="
    # Ch·ªâ lo·∫°i b·ªè c√°c chu·ªói s·ªë ph·ª©c t·∫°p (c√≥ d·∫•u ph·∫©y ho·∫∑c l·∫∑p l·∫°i), gi·ªØ l·∫°i s·ªë ƒë∆°n l·∫ª
    text = re.sub(r'\b\d{1,3}(?:,\d{1,3})+(?:\.\d+)?\b', ' ', text)  # Lo·∫°i b·ªè "1,2,3" nh∆∞ng gi·ªØ "5", "6"
    text = re.sub(r'\|{2,}', ' ', text)  # Lo·∫°i b·ªè c√°c chu·ªói "| | |"

    # Lo·∫°i b·ªè c√°c chu·ªói l·∫∑p l·∫°i
    matches = re.findall(r'(\b\w+\b(?:\s+\b\w+\b)*\s*\\?&?\s*\b\w+\b)(?:\s*\1)+', text)
    for match in matches:
        logging.info(f"Ph√°t hi·ªán chu·ªói l·∫∑p l·∫°i: '{match}'")
        # Thay th·∫ø chu·ªói l·∫∑p l·∫°i b·∫±ng m·ªôt l·∫ßn xu·∫•t hi·ªán duy nh·∫•t
        text = re.sub(r'(\b' + re.escape(match) + r'\s*)+', match + ' ', text)

    # S·ª≠a l·ªói OCR ti·∫øng Vi·ªát: chu·∫©n h√≥a k√Ω t·ª± Unicode
    text = unicodedata.normalize('NFC', text)

    # B·∫£o v·ªá t·ª´ ng·ªØ chuy√™n ng√†nh
    for keyword in TELECOM_KEYWORDS:
        placeholder = f"__{hashlib.md5(keyword.encode()).hexdigest()}__"
        text = text.replace(keyword, placeholder)

    # S·ª≠a c√°c l·ªói OCR ph·ªï bi·∫øn v·ªÅ k√Ω t·ª± ti·∫øng Vi·ªát
    replacements = {
        r'\bCH√â P√ì\b': 'CH·∫æ ƒê·ªò',  # S·ª≠a "CH√â P√ì" th√†nh "CH·∫æ ƒê·ªò"
        r'\bVIƒñC\b': 'VI·ªÜC',     # S·ª≠a "VIƒñC" th√†nh "VI·ªÜC"
        r'\bT√ô\b': 'T·ª™',         # S·ª≠a "T√ô" th√†nh "T·ª™"
        r'\be\b': '·ªá',
        r'\bo\b': '·ªô',
        r'\bO\b': '·ªò',
        r'\bu\b': '·ª•',
        r'\bd\b': 'ƒë',
        r'\bD\b': 'ƒê',
        r'\ba\b': '·∫°',
        r'\bi\b': '·ªã',
    }
    for wrong, correct in replacements.items():
        text = re.sub(wrong, correct, text)

    # Kh√¥i ph·ª•c t·ª´ ng·ªØ chuy√™n ng√†nh
    for keyword in TELECOM_KEYWORDS:
        placeholder = f"__{hashlib.md5(keyword.encode()).hexdigest()}__"
        text = text.replace(placeholder, keyword)

    # Log th√¥ng tin tr∆∞·ªõc v√† sau khi x·ª≠ l√Ω
    if text != original_text:
        logging.info(f"VƒÉn b·∫£n tr∆∞·ªõc x·ª≠ l√Ω: {original_text[:100]}...")
        logging.info(f"VƒÉn b·∫£n sau x·ª≠ l√Ω: {text[:100]}...")

    return text.strip()

class DocumentProcessor:
    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 150):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
        )

    def load_and_split(self, pdf_file) -> List[str]:
        saved_filename = pdf_file.name
        saved_filepath = os.path.join(DOCUMENTS_DIR, saved_filename)

        if not os.path.exists(saved_filepath):
            with open(saved_filepath, "wb") as f:
                f.write(pdf_file.read())

        full_text = ""
        page_boundaries = []
        current_pos = 0
        try:
            with pdfplumber.open(saved_filepath) as pdf:
                for page_num, page in enumerate(pdf.pages, start=1):
                    # Th·ª≠ tr√≠ch xu·∫•t vƒÉn b·∫£n tr·ª±c ti·∫øp
                    text = page.extract_text() or ""
                    if not text.strip():
                        # N·∫øu kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c vƒÉn b·∫£n, th·ª≠ OCR
                        logging.info(f"Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c vƒÉn b·∫£n t·ª´ trang {page_num}, th·ª≠ OCR...")
                        try:
                            # Chuy·ªÉn trang th√†nh h√¨nh ·∫£nh v√† th·ª±c hi·ªán OCR
                            image = page.to_image(resolution=300).original
                            image = image.convert("RGB")
                            text = pytesseract.image_to_string(image, lang='vie')
                            text = text.strip()
                            logging.info(f"OCR th√†nh c√¥ng cho trang {page_num}, vƒÉn b·∫£n: {text[:100]}...")
                        except Exception as e:
                            logging.error(f"L·ªói OCR cho trang {page_num}: {str(e)}")
                            text = ""
                    tables = page.extract_tables()
                    table_text = ""
                    if tables:
                        table_text = "\n".join([",".join(str(cell) if cell is not None else "" for cell in row) for row in tables])  # S·ª≠a t·ª´ table th√†nh tables
                    page_content = (text + "\n" + table_text).strip()
                    if page_content:
                        page_content = preprocess_text(page_content)
                        full_text += page_content + "\n"
                        page_boundaries.append((current_pos, page_num))
                        current_pos += len(page_content) + 1
                    else:
                        logging.warning(f"Kh√¥ng c√≥ n·ªôi dung ·ªü trang {page_num} sau khi x·ª≠ l√Ω.")
        except Exception as e:
            logging.error(f"L·ªói x·ª≠ l√Ω PDF: {str(e)}")
            display_message(f"L·ªói x·ª≠ l√Ω PDF: {str(e)}", "error")
            return [], []

        if not full_text.strip():
            logging.error("Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c vƒÉn b·∫£n t·ª´ to√†n b·ªô PDF.")
            return [], []

        splits = self.text_splitter.create_documents([full_text])
        chunks = [chunk.page_content.strip() for chunk in splits if chunk.page_content.strip()]
        page_numbers = []

        for chunk in splits:
            chunk_text = chunk.page_content.strip()
            if not chunk_text:
                continue
            chunk_start = full_text.index(chunk_text)
            chunk_page = page_boundaries[0][1]
            for pos, page_num in page_boundaries:
                if chunk_start >= pos:
                    chunk_page = page_num
                else:
                    break
            page_numbers.append(chunk_page)

        return chunks, page_numbers

class SentenceEmbedding:
    def __init__(self, model_path: str = MODEL_PATH):
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        if not os.path.exists(model_path):
            display_message(f"M√¥ h√¨nh kh√¥ng t√¨m th·∫•y t·∫°i {model_path}.", "error")
            st.stop()
        try:
            self.model = SentenceTransformer(model_path)
        except Exception as e:
            display_message(f"L·ªói t·∫£i m√¥ h√¨nh sentence-transformers: {str(e)}", "error")
            st.stop()

    def __call__(self, texts: List[str]) -> List[List[float]]:
        try:
            embeddings = self.model.encode(texts, show_progress_bar=False, normalize_embeddings=True)
            return embeddings.tolist()
        except Exception as e:
            display_message(f"L·ªói t·∫°o embedding: {str(e)}", "error")
            return []

class ChromaDBManager:
    def __init__(self, persist_directory: str = CHROMA_DB_PATH):
        self.key_path = ENCRYPTION_KEY_PATH
        os.makedirs(os.path.dirname(self.key_path), exist_ok=True)
        if os.path.exists(self.key_path):
            with open(self.key_path, "rb") as f:
                self.key = f.read()
        else:
            self.key = Fernet.generate_key()
            with open(self.key_path, "wb") as f:
                f.write(self.key)
        self.cipher = Fernet(self.key)
        os.makedirs(persist_directory, exist_ok=True)
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.collection = self.client.get_or_create_collection(name="viettel_docs")

    def add(self, texts: List[str], embeddings: List[List[float]], filename: str, page_numbers: List[int]):
        encrypted_texts = [self.cipher.encrypt(text.encode()).decode() for text in texts]
        ids = [f"doc_{hashlib.md5((filename + str(i)).encode()).hexdigest()}" for i in range(len(texts))]
        metas = [{"source": f"ƒêo·∫°n {i+1}", "filename": filename, "page_number": page_numbers[i], "upload_date": datetime.now().isoformat()} for i in range(len(texts))]
        self.collection.add(
            ids=ids,
            documents=encrypted_texts,
            metadatas=metas,
            embeddings=embeddings,
        )

    def query(self, query_embedding: List[float], top_k: int = 7):
        res = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            include=["documents", "metadatas", "distances"]
        )
        res["documents"] = [[self.cipher.decrypt(doc.encode()).decode() for doc in docs] for docs in res["documents"]]
        distances = np.array(res["distances"])
        similarities = 1 - distances
        res["distances"] = similarities.tolist()
        logging.info(f"Cosine similarities for query: {res['distances']}")
        return res

    def list_documents(self):
        try:
            results = self.collection.get(include=["metadatas"])
            filenames = set(meta["filename"] for meta in results["metadatas"])
            return list(filenames)
        except Exception as e:
            display_message(f"L·ªói li·ªát k√™ t√†i li·ªáu: {str(e)}", "error")
            return []

class AnswerGenerator:
    def __init__(self, model_type: str, role: str = "Expert"):
        self.model_type = model_type
        self.role = role
        if model_type == "openai":
            try:
                self.model = ChatOpenAI(model="gpt-4o-mini")
            except Exception as e:
                display_message(f"L·ªói k·∫øt n·ªëi OpenAI: {str(e)}. Vui l√≤ng ki·ªÉm tra API key ho·∫∑c k·∫øt n·ªëi internet.", "error")
                st.stop()
        elif model_type == "ollama":
            if not check_ollama():
                display_message("Ollama server kh√¥ng ho·∫°t ƒë·ªông. Vui l√≤ng ch·∫°y 'ollama run llama3.2'.", "error")
                st.stop()
            self.model = ChatOllama(
                base_url="http://localhost:11434",
                model="llama3.2",
                temperature=0.3,
                max_tokens=2000
            )
        else:
            raise ValueError(f"Unsupported model type: {model_type}")

    def generate_answer(self, question: str, context: str, citations: str) -> str:
        prompt = PromptTemplate(
            input_variables=["question", "context", "citations", "role"],
            template="""B·∫°n l√† m·ªôt chuy√™n gia trong lƒ©nh v·ª±c vi·ªÖn th√¥ng v√† v·∫≠n h√†nh Data Center, tr·∫£ l·ªùi ·ªü m·ª©c ƒë·ªô {role}. D·ª±a tr√™n c√¢u h·ªèi, ng·ªØ c·∫£nh v√† tr√≠ch d·∫´n sau ƒë√¢y, h√£y cung c·∫•p c√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c v√† mang t√≠nh k·ªπ thu·∫≠t, s·ª≠ d·ª•ng thu·∫≠t ng·ªØ chuy√™n ng√†nh vi·ªÖn th√¥ng n·∫øu c√≥:

C√¢u h·ªèi: {question}

Ng·ªØ c·∫£nh: {context}

Tr√≠ch d·∫´n: {citations}

N·∫øu ng·ªØ c·∫£nh ho·∫∑c tr√≠ch d·∫´n kh√¥ng ch·ª©a th√¥ng tin li√™n quan tr·ª±c ti·∫øp ƒë·∫øn c√¢u h·ªèi, ho·∫∑c th√¥ng tin kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c, h√£y tr·∫£ l·ªùi r√µ r√†ng: "Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong t√†i li·ªáu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y." ƒê·ª´ng c·ªë g·∫Øng suy di·ªÖn ho·∫∑c tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin kh√¥ng r√µ r√†ng.

N·∫øu c√≥ th√¥ng tin l·∫∑p l·∫°i trong ng·ªØ c·∫£nh ho·∫∑c tr√≠ch d·∫´n, h√£y ch·ªâ s·ª≠ d·ª•ng m·ªôt l·∫ßn v√† tr√¨nh b√†y c√¢u tr·∫£ l·ªùi m·∫°ch l·∫°c.

Tr·∫£ l·ªùi:"""
        )
        try:
            formatted_prompt = prompt.format(question=question, context=context, citations=citations, role=self.role)
            response = self.model.invoke(formatted_prompt)
            return response.content.strip()
        except Exception as e:
            display_message(f"L·ªói t·∫°o c√¢u tr·∫£ l·ªùi: {str(e)}", "error")
            return ""

def get_session_history(session_id: str):
    return SQLChatMessageHistory(session_id=session_id, connection=f"sqlite:///{HISTORY_DB_PATH}")

def wrap_text(text, width, canvas_obj, font_name="Helvetica", font_size=12):
    lines = []
    current_line = ""
    canvas_obj.setFont(font_name, font_size)
    for word in text.split():
        test_line = current_line + word + " "
        if canvas_obj.stringWidth(test_line, font_name, font_size) <= width:
            current_line = test_line
        else:
            lines.append(current_line.strip())
            current_line = word + " "
    if current_line:
        lines.append(current_line.strip())
    return lines

def export_history_to_pdf(history, font_name="Helvetica", font_size=12):
    buffer = io.BytesIO()
    c = canvas.Canvas(buffer, pagesize=letter)
    width, height = letter

    font_available = os.path.exists("DejaVuSans.ttf")
    if font_available:
        try:
            pdfmetrics.registerFont(TTFont("DejaVuSans", "DejaVuSans.ttf"))
            font_name = "DejaVuSans"
        except Exception as e:
            display_message(f"L·ªói t·∫£i font DejaVuSans: {str(e)}. S·ª≠ d·ª•ng font m·∫∑c ƒë·ªãnh.", "warning")
    else:
        display_message("Kh√¥ng t√¨m th·∫•y DejaVuSans.ttf. S·ª≠ d·ª•ng font m·∫∑c ƒë·ªãnh.", "warning")

    y_position = height - inch
    c.setFont(font_name, 16)
    c.drawString(inch, y_position, "L·ªãch s·ª≠ truy v·∫•n")
    y_position -= 0.5 * inch

    c.setFont(font_name, font_size)
    for i in range(0, len(history), 2):
        if i + 1 < len(history):
            user_msg = history[i].content
            ai_msg = history[i + 1].content

            c.setFont(font_name, font_size)
            c.drawString(inch, y_position, "C√¢u h·ªèi:")
            y_position -= 0.3 * inch
            for line in wrap_text(user_msg, width - 2 * inch, c, font_name, font_size):
                if y_position < inch:
                    c.showPage()
                    c.setFont(font_name, font_size)
                    y_position = height - inch
                c.drawString(inch, y_position, line)
                y_position -= 0.3 * inch

            y_position -= 0.3 * inch
            c.setFont(font_name, font_size)
            c.drawString(inch, y_position, "C√¢u tr·∫£ l·ªùi:")
            y_position -= 0.3 * inch
            for line in wrap_text(ai_msg, width - 2 * inch, c, font_name, font_size):
                if y_position < inch:
                    c.showPage()
                    c.setFont(font_name, font_size)
                    y_position = height - inch
                c.drawString(inch, y_position, line)
                y_position -= 0.3 * inch

            y_position -= 0.5 * inch
            if y_position < inch:
                c.showPage()
                y_position = height - inch

    c.save()
    buffer.seek(0)
    return buffer

def main():
    st.set_page_config(page_title="RAG ƒêi·ªán Vi·ªÖn Th√¥ng (N·ªôi b·ªô)", layout="wide")

    if "message_placeholder" not in st.session_state:
        st.session_state.message_placeholder = st.empty()

    st.title("üìÑ H·ªá th·ªëng truy v·∫•n t√†i li·ªáu th√¥ng minh")

    if "authenticated" not in st.session_state:
        st.session_state.authenticated = False
    if not st.session_state.authenticated:
        password = st.text_input("Nh·∫≠p m·∫≠t kh·∫©u:", type="password", value="")
        if st.button("X√°c th·ª±c"):
            if password == "T0mmy":
                st.session_state.authenticated = True
                st.rerun()
            else:
                display_message("M·∫≠t kh·∫©u kh√¥ng ƒë√∫ng. Vui l√≤ng th·ª≠ l·∫°i.", "error")
        return

    has_db = check_existing_data()
    # Kh·ªüi t·∫°o c√°c ƒë·ªëi t∆∞·ª£ng n·∫øu ch∆∞a t·ªìn t·∫°i
    if "processor" not in st.session_state:
        st.session_state.processor = DocumentProcessor()
    if "embedder" not in st.session_state:
        st.session_state.embedder = SentenceEmbedding(model_path=MODEL_PATH)
    if "chroma" not in st.session_state:
        st.session_state.chroma = ChromaDBManager(persist_directory=CHROMA_DB_PATH)
    if "query_history" not in st.session_state:
        st.session_state.query_history = get_session_history("user_default")
    if "has_db_notified" not in st.session_state:
        st.session_state.has_db_notified = False

    st.sidebar.header("C·∫•u h√¨nh")
    llm_type = st.sidebar.radio(
        "Ch·ªçn m√¥ h√¨nh LLM:",
        ["ollama", "openai"],
        format_func=lambda x: "Ollama Llama3.2 (Offline)" if x == "ollama" else "OpenAI GPT-4o-mini (Online)",
    )
    role = st.sidebar.radio("M·ª©c ƒë·ªô chi ti·∫øt:", ["Beginner", "Expert", "PhD"])
    similarity_threshold = st.sidebar.slider("Ng∆∞·ª°ng ƒë·ªô t∆∞∆°ng ƒë·ªìng (cosine similarity)", 0.0, 1.0, 0.6, step=0.05)

    if has_db:
        display_message("CSDL ƒë√£ c√≥ d·ªØ li·ªáu. B·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu truy v·∫•n.", "info")
        st.session_state.has_db_notified = False
    else:
        if not st.session_state.has_db_notified:
            display_message("Ch∆∞a c√≥ d·ªØ li·ªáu. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu PDF.", "warning")
            st.session_state.has_db_notified = True

    st.sidebar.subheader("Qu·∫£n l√Ω t√†i li·ªáu")
    pdf_file = st.sidebar.file_uploader("T·∫£i l√™n file PDF", type="pdf")
    if pdf_file:
        with st.spinner("ƒêang x·ª≠ l√Ω t√†i li·ªáu‚Ä¶"):
            try:
                chunks, page_numbers = st.session_state.processor.load_and_split(pdf_file)
                if not chunks:
                    display_message("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ PDF.", "error")
                    return
                # Ki·ªÉm tra n·ªôi dung b·ªã c·∫Øt b·ªõt (d·ª±a tr√™n k√≠ch th∆∞·ªõc chunks)
                total_length = sum(len(chunk) for chunk in chunks)
                if total_length > 1000000:  # Gi·∫£ s·ª≠ gi·ªõi h·∫°n 1 tri·ªáu k√Ω t·ª±
                    display_message("N·ªôi dung t√†i li·ªáu c√≥ th·ªÉ b·ªã c·∫Øt b·ªõt do k√≠ch th∆∞·ªõc l·ªõn. M·ªôt s·ªë th√¥ng tin c√≥ th·ªÉ kh√¥ng ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß.", "warning")
                embeddings = st.session_state.embedder(chunks)
                st.session_state.chroma.add(chunks, embeddings, filename=pdf_file.name, page_numbers=page_numbers)
                display_message("ƒê√£ x·ª≠ l√Ω v√† l∆∞u t√†i li·ªáu th√†nh c√¥ng!", "info")
                st.session_state.has_db_notified = False
            except Exception as e:
                display_message(f"L·ªói x·ª≠ l√Ω t√†i li·ªáu: {str(e)}", "error")

    if has_db:
        st.sidebar.subheader("T√†i li·ªáu ƒë√£ t·∫£i")
        if "chroma" in st.session_state:
            documents = st.session_state.chroma.list_documents()
            if documents:
                for doc in documents:
                    st.sidebar.write(f"- {doc}")
            else:
                st.sidebar.write("Ch∆∞a c√≥ t√†i li·ªáu n√†o.")
        else:
            st.sidebar.write("Ch∆∞a kh·ªüi t·∫°o ChromaDB.")

    if has_db and st.sidebar.button("X√≥a t·∫•t c·∫£ t√†i li·ªáu"):
        try:
            if "chroma" in st.session_state:
                st.session_state.chroma.client.delete_collection("viettel_docs")
                st.session_state.chroma = None
                st.session_state.embedder = None
                st.session_state.processor = None
                for file in os.listdir(DOCUMENTS_DIR):
                    file_path = os.path.join(DOCUMENTS_DIR, file)
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                display_message("ƒê√£ x√≥a t·∫•t c·∫£ t√†i li·ªáu!", "info")
                st.session_state.has_db_notified = False
            else:
                display_message("Ch∆∞a kh·ªüi t·∫°o ChromaDB.", "error")
        except Exception as e:
            display_message(f"L·ªói x√≥a t√†i li·ªáu: {str(e)}", "error")

    query = st.text_input("Nh·∫≠p c√¢u h·ªèi (ti·∫øng Vi·ªát):")
    if query:
        if "chroma" not in st.session_state or st.session_state.chroma is None or "embedder" not in st.session_state:
            display_message("Ch∆∞a c√≥ d·ªØ li·ªáu. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu PDF.", "warning")
            return
        with st.spinner("ƒêang t·∫°o c√¢u tr·∫£ l·ªùi‚Ä¶"):
            try:
                @lru_cache(maxsize=100)
                def cached_query(query: str, top_k: int) -> tuple:
                    query = preprocess_text(query)
                    emb = st.session_state.embedder([query])[0]
                    res = st.session_state.chroma.query(emb, top_k)
                    return res["documents"][0], res["metadatas"][0], res["distances"][0]

                docs, metas, dists = cached_query(query, top_k=7)
                relevant_docs = []
                relevant_metas = []
                relevant_dists = []
                for doc, meta, dist in zip(docs, metas, dists):
                    if dist >= similarity_threshold:
                        relevant_docs.append(doc)
                        relevant_metas.append(meta)
                        relevant_dists.append(dist)

                citations_list = []
                if relevant_docs:
                    context = " ".join(relevant_docs)
                    for meta, doc, dist in zip(relevant_metas, relevant_docs, relevant_dists):
                        citation = {
                            "source": meta["source"],
                            "filename": meta["filename"],
                            "page_number": meta.get("page_number", "Kh√¥ng x√°c ƒë·ªãnh"),
                            "score": dist,
                            "content": doc
                        }
                        citations_list.append(citation)
                    citations = "C√≥ c√°c tr√≠ch d·∫´n li√™n quan."
                else:
                    context = "Kh√¥ng c√≥ th√¥ng tin ph√π h·ª£p trong t√†i li·ªáu."
                    citations = f"Kh√¥ng c√≥ tr√≠ch d·∫´n n√†o ƒë·∫°t ng∆∞·ª°ng t∆∞∆°ng ƒë·ªìng (cosine similarity >= {similarity_threshold})."

                answer_generator = AnswerGenerator(model_type=llm_type, role=role)
                final_answer = answer_generator.generate_answer(query, context, citations)

                if final_answer.startswith("Error generating answer"):
                    display_message(final_answer, "error")
                    return

                st.session_state.query_history.add_user_message(query)
                st.session_state.query_history.add_ai_message(final_answer)

                st.markdown("### C√¢u tr·∫£ l·ªùi cu·ªëi c√πng:")
                st.write(final_answer)

                with st.expander("üìö Xem tr√≠ch d·∫´n ngu·ªìn"):
                    if citations_list:
                        for idx, citation in enumerate(citations_list, 1):
                            st.markdown(f"#### Tr√≠ch d·∫´n {idx}:")
                            st.markdown(f"- **Ngu·ªìn**: {citation['source']}")
                            st.markdown(f"- **T√†i li·ªáu**: {citation['filename']}")
                            st.markdown(f"- **Trang**: {citation['page_number']}")
                            st.markdown(f"- **ƒê·ªô t∆∞∆°ng ƒë·ªìng (cosine similarity)**: {citation['score']:.4f}")
                            st.markdown(f"- **N·ªôi dung**:")
                            st.write(citation['content'])
                            st.markdown("---")
                    else:
                        st.write(f"Kh√¥ng c√≥ tr√≠ch d·∫´n n√†o ƒë·∫°t ng∆∞·ª°ng t∆∞∆°ng ƒë·ªìng (cosine similarity >= {similarity_threshold}).")

            except Exception as e:
                display_message(f"L·ªói truy v·∫•n: {str(e)}", "error")

    with st.expander("üìú L·ªãch s·ª≠ truy v·∫•n"):
        messages = st.session_state.query_history.messages
        for i in range(0, len(messages), 2):
            if i + 1 < len(messages):
                user_msg = messages[i]
                ai_msg = messages[i + 1]
                user_content = user_msg.content.replace("\n", "<br>")
                ai_content = ai_msg.content.replace("\n", "<br>")
                st.markdown(
                    """
                    <div style="background-color: #ffffff; padding: 10px; border-radius: 5px; margin-bottom: 10px;">
                        üë§ <strong>Ng∆∞·ªùi d√πng:</strong> {user_content}
                    </div>
                    """.format(user_content=user_content),
                    unsafe_allow_html=True
                )
                st.markdown(
                    """
                    <div style="background-color: #d3d3d3; padding: 10px; border-radius: 5px; margin-bottom: 10px;">
                        ü§ñ <strong>Tr·ª£ l√Ω:</strong> {ai_content}
                    </div>
                    """.format(ai_content=ai_content),
                    unsafe_allow_html=True
                )
                if i + 2 < len(messages):
                    st.markdown("---")
            else:
                user_msg = messages[i]
                user_content = user_msg.content.replace("\n", "<br>")
                st.markdown(
                    """
                    <div style="background-color: #ffffff; padding: 10px; border-radius: 5px; margin-bottom: 10px;">
                        üë§ <strong>Ng∆∞·ªùi d√πng:</strong> {user_content}
                    </div>
                    """.format(user_content=user_content),
                    unsafe_allow_html=True
                )

        if messages:
            if st.button("Xu·∫•t l·ªãch s·ª≠ truy v·∫•n"):
                pdf_buffer = export_history_to_pdf(messages)
                st.download_button(
                    "T·∫£i l·ªãch s·ª≠ truy v·∫•n (PDF)",
                    pdf_buffer,
                    file_name="query_history.pdf",
                    mime="application/pdf"
                )

if __name__ == "__main__":
    main()